{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Spark notebook will answer the questions of the Amadeus Challenge (bookings + searches) using Spark instead of python pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) First exercise: count the number of lines in Python for each file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create manually the SparkContext (this is not needed, by default sc is loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.SparkConf;\n",
    "import org.apache.spark.SparkContext;\n",
    "import org.apache.spark.SparkContext._\n",
    "//val conf = new SparkConf().setMaster(\"local[*]\").setAppName(\"amadeus challenge\")//all cores in local\n",
    "val conf = new SparkConf().setMaster(\"local[*]\").setAppName(\"amadeus challenge\");//all cores in local\n",
    "val sc = new SparkContext(conf);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read files\n",
    "Define paths and load RDDs on searches and bookings, save first line as header then remove first lines from RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "root_path = /media/sf_VirtualFolder/amadeus/\n",
       "searches_path = /media/sf_VirtualFolder/amadeus/searches.csv.bz2\n",
       "bookings_path = /media/sf_VirtualFolder/amadeus/bookings.csv.bz2\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "/media/sf_VirtualFolder/amadeus/bookings.csv.bz2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val root_path: String = \"/media/sf_VirtualFolder/amadeus/\";//where to find files\n",
    "val searches_path: String = root_path + \"searches.csv.bz2\";\n",
    "val bookings_path: String = root_path + \"bookings.csv.bz2\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20390199                                                                        \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "searches = /media/sf_VirtualFolder/amadeus/searches.csv.bz2 MapPartitionsRDD[1] at textFile at <console>:36\n",
       "searches_count = 20390199\n",
       "headerColumns_searches = List(Date, Time, TxnCode, OfficeID, Country, Origin, Destination, RoundTrip, NbSegments, Seg1Departure, Seg1Arrival, Seg1Date, Seg1Carrier, Seg1BookingCode, Seg2Departure, Seg2Arrival, Seg2Date, Seg2Carrier, Seg2BookingCode, Seg3Departure, Seg3Arrival, Seg3Date, Seg3Carrier, Seg3BookingCode, Seg4Departure, Seg4Arrival, Seg4Date, Seg4Carrier, Seg4BookingCode, Seg5Departure, Seg5Arrival, Seg5Date, Seg5Carrier, Seg5BookingCode, Seg6Departure, Seg6Arrival, Seg6Date, Seg6Carrier, Seg6BookingCode, From, IsPublishedForNeg, IsFromInternet, IsFromVista, TerminalID, InternetOffice)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "head_...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "List(Date, Time, TxnCode, OfficeID, Country, Origin, Destination, RoundTrip, NbSegments, Seg1Departure, Seg1Arrival, Seg1Date, Seg1Carrier, Seg1BookingCode, Seg2Departure, Seg2Arrival, Seg2Date, Seg2Carrier, Seg2BookingCode, Seg3Departure, Seg3Arrival, Seg3Date, Seg3Carrier, Seg3BookingCode, Seg4Departure, Seg4Arrival, Seg4Date, Seg4Carrier, Seg4BookingCode, Seg5Departure, Seg5Arrival, Seg5Date, Seg5Carrier, Seg5BookingCode, Seg6Departure, Seg6Arrival, Seg6Date, Seg6Carrier, Seg6BookingCode, From, IsPublishedForNeg, IsFromInternet, IsFromVista, TerminalID, InternetOffice)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val searches = sc.textFile(searches_path)//Load RDD 1\n",
    "val searches_count = searches.count()\n",
    "println(searches_count)\n",
    "val headerColumns_searches = searches.first().split(\"\\\\^\").to[List]\n",
    "val head_searches = headerColumns_searches.map(field => field.trim().toLowerCase())\n",
    "//Now drop first line, split lines with ^ separator and trim words\n",
    "val rdd_searches = searches.mapPartitionsWithIndex { (idx, iter) => if (idx == 0) iter.drop(1) else iter }.map(line=>line.split(\"\\\\^\").to[List])\n",
    "                                                                                                               //.map(word=>word.trim()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000011                                                                        \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "bookings = /media/sf_VirtualFolder/amadeus/bookings.csv.bz2 MapPartitionsRDD[5] at textFile at <console>:36\n",
       "bookings_count = 10000011\n",
       "headerColumns_bookings = List(\"act_date           \", source, pos_ctry, pos_iata, \"pos_oid  \", \"rloc          \", \"cre_date           \", duration, distance, dep_port, dep_city, dep_ctry, arr_port, arr_city, arr_ctry, lst_port, lst_city, lst_ctry, brd_port, brd_city, brd_ctry, off_port, off_city, off_ctry, mkt_port, mkt_city, mkt_ctry, intl, \"route          \", carrier, bkg_class, cab_class, \"brd_time           \", \"off_time           \", pax, year, month, \"oid      \")\n",
       "head_bookings = List(act_date, source, pos_ctry, pos_iata, pos_oid, rloc, cre_date, duration, distance, dep_port, dep_city, dep...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "List(act_date, source, pos_ctry, pos_iata, pos_oid, rloc, cre_date, duration, distance, dep_port, dep_city, dep_ctry, arr_port, arr_city, arr_ctry, lst_port, lst_city, lst_ctry, brd_port, brd_city, brd_ctry, off_port, off_city, off_ctry, mkt_port, mkt_city, mkt_ctry, intl, route, carrier, bkg_class, cab_class, brd_time, off_time, pax, year, month, oid)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val bookings = sc.textFile(bookings_path);//Load RDD 2\n",
    "val bookings_count = bookings.count()\n",
    "println(bookings_count)\n",
    "val headerColumns_bookings = bookings.first().split(\"\\\\^\").to[List];\n",
    "val head_bookings = headerColumns_bookings.map(field => field.trim().toLowerCase())\n",
    "val rdd_bookings = bookings.mapPartitionsWithIndex { (idx, iter) => if (idx == 0) iter.drop(1) else iter }.map(line=>line.split(\"\\\\^\").to[List])\n",
    "//.map(word=>word.trim()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ind_bookings: (field: String)Int\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "34\n"
     ]
    }
   ],
   "source": [
    "def ind_bookings(field: String): Int = headerColumns_bookings.indexOf(field) \n",
    "println(ind_bookings(\"arr_port\"))\n",
    "println(ind_bookings(\"pax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_pair_ex1 = MapPartitionsRDD[8] at map at <console>:39\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[8] at map at <console>:39"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df_pair_ex1 = rdd_bookings.map(l => (l(12),l(34)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ex1 = ShuffledRDD[11] at groupByKey at <console>:41\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ShuffledRDD[11] at groupByKey at <console>:41"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ex1 = df_pair_ex1.filter((_._2.toInt < 0) ).filter(_._2.toInt >= 0).groupByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ex11 = MapPartitionsRDD[12] at mapValues at <console>:43\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[12] at mapValues at <console>:43"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ex11 = ex1.mapValues(v=>(v.map(x => x.toInt)).sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 4:===========================>                              (8 + 1) / 17]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.SparkException\n",
       "Message: Job aborted due to stage failure: Task 8 in stage 4.0 failed 1 times, most recent failure: Lost task 8.0 in stage 4.0 (TID 42, localhost, executor driver): java.lang.IndexOutOfBoundsException: 34\n",
       "\tat scala.collection.LinearSeqOptimized$class.apply(LinearSeqOptimized.scala:65)\n",
       "\tat scala.collection.immutable.List.apply(List.scala:84)\n",
       "\tat $line44.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(<console>:39)\n",
       "\tat $line44.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(<console>:39)\n",
       "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n",
       "\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462)\n",
       "\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461)\n",
       "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
       "\tat java.lang.Thread.run(Thread.java:748)\n",
       "\n",
       "Driver stacktrace:\n",
       "StackTrace: \tat scala.collection.LinearSeqOptimized$class.apply(LinearSeqOptimized.scala:65)\n",
       "\tat scala.collection.immutable.List.apply(List.scala:84)\n",
       "\tat $anonfun$1.apply(<console>:39)\n",
       "\tat $anonfun$1.apply(<console>:39)\n",
       "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n",
       "\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462)\n",
       "\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461)\n",
       "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
       "\tat java.lang.Thread.run(Thread.java:748)\n",
       "Driver stacktrace:\n",
       "  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n",
       "  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
       "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n",
       "  at scala.Option.foreach(Option.scala:257)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n",
       "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1353)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n",
       "  at org.apache.spark.rdd.RDD.take(RDD.scala:1326)\n",
       "  ... 44 elided\n",
       "Caused by: java.lang.IndexOutOfBoundsException: 34\n",
       "  at scala.collection.LinearSeqOptimized$class.apply(LinearSeqOptimized.scala:65)\n",
       "  at scala.collection.immutable.List.apply(List.scala:84)\n",
       "  at $anonfun$1.apply(<console>:39)\n",
       "  at $anonfun$1.apply(<console>:39)\n",
       "  at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n",
       "  at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462)\n",
       "  at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461)\n",
       "  at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)\n",
       "  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n",
       "  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
       "  at org.apache.spark.scheduler.Task.run(Task.scala:99)\n",
       "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ex12 = ex11.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check reading was correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n",
      "List(date, time, txncode, officeid, country, origin, destination, roundtrip, nbsegments, seg1departure, seg1arrival, seg1date, seg1carrier, seg1bookingcode, seg2departure, seg2arrival, seg2date, seg2carrier, seg2bookingcode, seg3departure, seg3arrival, seg3date, seg3carrier, seg3bookingcode, seg4departure, seg4arrival, seg4date, seg4carrier, seg4bookingcode, seg5departure, seg5arrival, seg5date, seg5carrier, seg5bookingcode, seg6departure, seg6arrival, seg6date, seg6carrier, seg6bookingcode, from, ispublishedforneg, isfrominternet, isfromvista, terminalid, internetoffice)\n",
      "List(2013-01-01, 20:25:57, MPT, 624d8c3ac0b3a7ca03e3c167e0f48327, DE, TXL, AUH, 1, 2, TXL, AUH, 2013-01-26, D2, , AUH, TXL, 2013-02-02, D2, , , , , , , , , , , , , , , , , , , , , , 1ASIWS, 0, 0, 0, d41d8cd98f00b204e9800998ecf8427e, FRA)\n"
     ]
    }
   ],
   "source": [
    "println(rdd_searches.first().length==head_searches.length)\n",
    "println(head_searches)\n",
    "println(rdd_searches.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n",
      "List(act_date, source, pos_ctry, pos_iata, pos_oid, rloc, cre_date, duration, distance, dep_port, dep_city, dep_ctry, arr_port, arr_city, arr_ctry, lst_port, lst_city, lst_ctry, brd_port, brd_city, brd_ctry, off_port, off_city, off_ctry, mkt_port, mkt_city, mkt_ctry, intl, route, carrier, bkg_class, cab_class, brd_time, off_time, pax, year, month, oid)\n",
      "List(2013-03-05 00:00:00, 1A, DE, a68dd7ae953c8acfb187a1af2dcbe123, 1a11ae49fcbf545fd2afc1a24d88d2b7, ea65900e72d71f4626378e2ebd298267, 2013-02-22 00:00:00, 1708, 0, ZRH, ZRH, CH, LHR, LON, GB, ZRH, ZRH, CH, LHR, LON, GB, ZRH, ZRH, CH, LHRZRH, LONZRH, CHGB, 1, LHRZRH, VI, T, Y, 2013-03-07 08:50:00, 2013-03-07 11:33:37, -1, 2013, 3, NULL)\n"
     ]
    }
   ],
   "source": [
    "println(rdd_bookings.first().length==head_bookings.length)\n",
    "println(head_bookings)\n",
    "println(rdd_bookings.first())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Count number of lines on files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20390199\n",
      "10000011\n"
     ]
    }
   ],
   "source": [
    "//Print number of lines\n",
    "println(searches_count)\n",
    "println(bookings_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This takes a long time to run (about 4 minutes each counting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I have some problem with the code above and It is a bit inefficient to work with such a big RDD, instead we would read directly the columns we need for each task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Read first rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "first_searches = Date^Time^TxnCode^OfficeID^Country^Origin^Destination^RoundTrip^NbSegments^Seg1Departure^Seg1Arrival^Seg1Date^Seg1Carrier^Seg1BookingCode^Seg2Departure^Seg2Arrival^Seg2Date^Seg2Carrier^Seg2BookingCode^Seg3Departure^Seg3Arrival^Seg3Date^Seg3Carrier^Seg3BookingCode^Seg4Departure^Seg4Arrival^Seg4Date^Seg4Carrier^Seg4BookingCode^Seg5Departure^Seg5Arrival^Seg5Date^Seg5Carrier^Seg5BookingCode^Seg6Departure^Seg6Arrival^Seg6Date^Seg6Carrier^Seg6BookingCode^From^IsPublishedForNeg^IsFromInternet^IsFromVista^TerminalID^InternetOffice\n",
       "first_bookings = \"act_date           ^source^pos_ctry^pos_iata^pos_oid  ^rloc          ^cre_date           ^duration^distance^dep_port^dep_city^dep_ctry^arr_port^arr_city^arr_ctry^lst_port^lst_city^lst_ctry^brd_port^brd_city^brd_ctry^o...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "act_date           ^source^pos_ctry^pos_iata^pos_oid  ^rloc          ^cre_date           ^duration^distance^dep_port^dep_city^dep_ctry^arr_port^arr_city^arr_ctry^lst_port^lst_city^lst_ctry^brd_port^brd_city^brd_ctry^off_port^off_city^off_ctry^mkt_port^mkt_city^mkt_ctry^intl^route          ^carrier^bkg_class^cab_class^brd_time           ^off_time           ^pax^year^month^oid      "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val first_searches = sc.textFile(searches_path).first//Take only first row and c\n",
    "val first_bookings = sc.textFile(bookings_path).first//Take only first row and c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the separator is the character \"\\\\^\". So now we will create a list with the trimmed field names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lf_searches = List(Date, Time, TxnCode, OfficeID, Country, Origin, Destination, RoundTrip, NbSegments, Seg1Departure, Seg1Arrival, Seg1Date, Seg1Carrier, Seg1BookingCode, Seg2Departure, Seg2Arrival, Seg2Date, Seg2Carrier, Seg2BookingCode, Seg3Departure, Seg3Arrival, Seg3Date, Seg3Carrier, Seg3BookingCode, Seg4Departure, Seg4Arrival, Seg4Date, Seg4Carrier, Seg4BookingCode, Seg5Departure, Seg5Arrival, Seg5Date, Seg5Carrier, Seg5BookingCode, Seg6Departure, Seg6Arrival, Seg6Date, Seg6Carrier, Seg6BookingCode, From, IsPublishedForNeg, IsFromInternet, IsFromVista, TerminalID, InternetOffice)\n",
       "lf_bookings = List(act_date, source, pos_ctry, pos_iata, pos_oid, rloc, cre_date, duration, distance, dep_port, dep_city, dep_ctry, arr_port, arr_city, arr_ctry, lst_port, lst_...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "List(act_date, source, pos_ctry, pos_iata, pos_oid, rloc, cre_date, duration, distance, dep_port, dep_city, dep_ctry, arr_port, arr_city, arr_ctry, lst_port, lst_city, lst_ctry, brd_port, brd_city, brd_ctry, off_port, off_city, off_ctry, mkt_port, mkt_city, mkt_ctry, intl, route, carrier, bkg_class, cab_class, brd_time, off_time, pax, year, month, oid)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lf_searches = first_searches.split(\"\\\\^\").map(field=>field.trim).to[List]\n",
    "val lf_bookings = first_bookings.split(\"\\\\^\").map(field=>field.trim).to[List]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will create a dictionary to be able to refer to the column just by using the field name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dsearch = Map(TxnCode -> 2, Seg6Departure -> 34, TerminalID -> 43, Country -> 4, Seg1Carrier -> 12, Seg3Departure -> 19, Seg5Departure -> 29, Origin -> 5, RoundTrip -> 7, OfficeID -> 3, Seg2Arrival -> 15, InternetOffice -> 44, Seg5Carrier -> 32, Seg3Date -> 21, Seg5BookingCode -> 33, Seg2Departure -> 14, Seg1BookingCode -> 13, Seg4Departure -> 24, Seg6Carrier -> 37, Seg6Arrival -> 35, IsFromVista -> 42, Seg4Date -> 26, IsPublishedForNeg -> 40, Seg4Arrival -> 25, IsFromInternet -> 41, Seg1Arrival -> 10, Date -> 0, Seg2Carrier -> 17, Seg4Carrier -> 27, Seg6BookingCode -> 38, Seg3Arrival -> 20, NbSegments -> 8, Destination -> 6, Seg1Departure -> 9, Seg1Date -> 11, Seg3BookingCode -> 23, Seg3Carrier -> 22, Seg5Date -> 31, Seg2BookingCode -> 18, Seg5Arrival -> 30, Seg6Date -...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map(TxnCode -> 2, Seg6Departure -> 34, TerminalID -> 43, Country -> 4, Seg1Carrier -> 12, Seg3Departure -> 19, Seg5Departure -> 29, Origin -> 5, RoundTrip -> 7, OfficeID -> 3, Seg2Arrival -> 15, InternetOffice -> 44, Seg5Carrier -> 32, Seg3Date -> 21, Seg5BookingCode -> 33, Seg2Departure -> 14, Seg1BookingCode -> 13, Seg4Departure -> 24, Seg6Carrier -> 37, Seg6Arrival -> 35, IsFromVista -> 42, Seg4Date -> 26, IsPublishedForNeg -> 40, Seg4Arrival -> 25, IsFromInternet -> 41, Seg1Arrival -> 10, Date -> 0, Seg2Carrier -> 17, Seg4Carrier -> 27, Seg6BookingCode -> 38, Seg3Arrival -> 20, NbSegments -> 8, Destination -> 6, Seg1Departure -> 9, Seg1Date -> 11, Seg3BookingCode -> 23, Seg3Carrier -> 22, Seg5Date -> 31, Seg2BookingCode -> 18, Seg5Arrival -> 30, Seg6Date -> 36, From -> 39, Time -> 1, Seg4BookingCode -> 28, Seg2Date -> 16)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dsearch: Map[String, Int] = Map(lf_searches map {s => (s, lf_searches.indexOf(s))} : _*);//searchs dictionary\n",
    "val dbook: Map[String, Int] = Map(lf_bookings map {s => (s, lf_bookings.indexOf(s))} : _*);//bookings dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbook(\"pax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbook(\"arr_port\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Top 10 arrival airports in the world in 2013 (using the bookings file)\n",
    "Arrival airport is the column arr_port. It is the IATA code for the airport\n",
    "\n",
    "To get the total number of passengers for an airport, you can sum the column pax, grouping by arr_port. Note that there is negative pax. That corresponds to cancelations. So to get the total number of passengers that have actually booked, you should sum including the negatives (that will remove the canceled bookings).\n",
    "\n",
    "Print the top 10 arrival airports in the standard output, including the number of passengers.\n",
    "\n",
    "Bonus point: Get the name of the city or airport corresponding to that airport (programatically, we suggest to have a look at GeoBases in Github)\n",
    "\n",
    "Bonus point: Solve this problem using pandas (instead of any other approach)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can read bookings file selecting for instance the columns of arr_port and pax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.{RangePartitioner,HashPartitioner, SparkContext}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! now we can use the transformations and methods for Pair RDDs!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "head1 = \"act_date           ^source^pos_ctry^pos_iata^pos_oid  ^rloc          ^cre_date           ^duration^distance^dep_port^dep_city^dep_ctry^arr_port^arr_city^arr_ctry^lst_port^lst_city^lst_ctry^brd_port^brd_city^brd_ctry^off_port^off_city^off_ctry^mkt_port^mkt_city^mkt_ctry^intl^route          ^carrier^bkg_class^cab_class^brd_time           ^off_time           ^pax^year^month^oid      \"\n",
       "rdd1 = MapPartitionsRDD[31] at map at <console>:41\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[31] at map at <console>:41"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Now drop first line, split lines with ^ separator and trim words\n",
    "val head1 = sc.textFile(bookings_path).first\n",
    "val rdd1 = sc.textFile(bookings_path).filter(_!=head1).map(line => (line.split(\"\\\\^\")(12).trim, line.split(\"\\\\^\")(34).toInt)).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 31:>                                                         (0 + 4) / 4]"
     ]
    }
   ],
   "source": [
    "\n",
    "rdd1.take(10000010).last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 22:======>                                                  (2 + 8) / 17]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lastException = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.SparkException\n",
       "Message: Job aborted due to stage failure: Task 8 in stage 22.0 failed 1 times, most recent failure: Lost task 8.0 in stage 22.0 (TID 82, localhost, executor driver): java.lang.ArrayIndexOutOfBoundsException\n",
       "\n",
       "Driver stacktrace:\n",
       "StackTrace:   at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n",
       "  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
       "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n",
       "  at scala.Option.foreach(Option.scala:257)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n",
       "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1958)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$foreach$1.apply(RDD.scala:917)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$foreach$1.apply(RDD.scala:915)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n",
       "  at org.apache.spark.rdd.RDD.foreach(RDD.scala:915)\n",
       "  ... 44 elided"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//val rdd3 = rdd1.filter(_._1!=null).filter(_._2!= null).map(x=>(x._1.trim().toUpperCase(), x._2.toInt)).reduceByKey(_+_)\n",
    "val rdd3 = rdd1.filter(_._1.length>0).filter(_._2!=null)\n",
    "//rdd1.filter(_._1 !=null).foreach(println)\n",
    "//val kk = rdd3.filter(_._2>10).take(10)\n",
    "\n",
    "//val rdd4 = rdd2.reduceByKey((x,y) => x + y )\n",
    "//.mapByValue(_+_)\n",
    "//.groupByKey\n",
    "//\n",
    "//rdd4.take(10)\n",
    "//val top10 = rdd3.sortByKey(false).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10.forEach(println(_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 17:===>                                                     (1 + 8) / 17]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.SparkException\n",
       "Message: Job aborted due to stage failure: Task 8 in stage 17.0 failed 1 times, most recent failure: Lost task 8.0 in stage 17.0 (TID 57, localhost, executor driver): java.lang.ArrayIndexOutOfBoundsException\n",
       "\n",
       "Driver stacktrace:\n",
       "StackTrace: Driver stacktrace:\n",
       "  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n",
       "  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
       "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n",
       "  at scala.Option.foreach(Option.scala:257)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n",
       "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1981)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1025)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n",
       "  at org.apache.spark.rdd.RDD.reduce(RDD.scala:1007)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$max$1.apply(RDD.scala:1441)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n",
       "  at org.apache.spark.rdd.RDD.max(RDD.scala:1440)\n",
       "  ... 44 elided"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Top 10 arrival airports in the world in 2013 (using the bookings file)\n",
    "Arrival airport is the column arr_port. It is the IATA code for the airport\n",
    "\n",
    "To get the total number of passengers for an airport, you can sum the column pax, grouping by arr_port. Note that there is negative pax. That corresponds to cancelations. So to get the total number of passengers that have actually booked, you should sum including the negatives (that will remove the canceled bookings).\n",
    "\n",
    "Print the top 10 arrival airports in the standard output, including the number of passengers.\n",
    "\n",
    "Bonus point: Get the name of the city or airport corresponding to that airport (programatically, we suggest to have a look at GeoBases in Github)\n",
    "\n",
    "Bonus point: Solve this problem using pandas (instead of any other approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
