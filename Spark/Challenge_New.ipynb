{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Spark notebook will answer the questions of the Amadeus Challenge (bookings + searches) using Spark instead of python pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.storage.StorageLevel._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) First exercise: count the number of lines in Python for each file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create manually the SparkContext (this is not needed, by default sc is loaded). Indeed if I try this I have problems..."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "import org.apache.spark.SparkConf;\n",
    "import org.apache.spark.SparkContext;\n",
    "import org.apache.spark.SparkContext._\n",
    "//val conf = new SparkConf().setMaster(\"local[*]\").setAppName(\"amadeus challenge\")//all cores in local\n",
    "val conf = new SparkConf().setMaster(\"local[4]\").setAppName(\"amadeus challenge\");//all cores in local\n",
    "val sc = new SparkContext(conf);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read files\n",
    "Define paths and load RDDs on searches and bookings, save first line as header then remove first lines from RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "root_path = /media/sf_VirtualFolder/amadeus/\n",
       "searches_path = /media/sf_VirtualFolder/amadeus/searches.csv.bz2\n",
       "bookings_path = /media/sf_VirtualFolder/amadeus/bookings.csv.bz2\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "/media/sf_VirtualFolder/amadeus/bookings.csv.bz2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val root_path: String = \"/media/sf_VirtualFolder/amadeus/\";//where to find files\n",
    "val searches_path: String = root_path + \"searches.csv.bz2\";\n",
    "val bookings_path: String = root_path + \"bookings.csv.bz2\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "searches = /media/sf_VirtualFolder/amadeus/searches.csv.bz2 MapPartitionsRDD[1] at textFile at <console>:34\n",
       "bookings = /media/sf_VirtualFolder/amadeus/bookings.csv.bz2 MapPartitionsRDD[3] at textFile at <console>:35\n",
       "headerColumns_searches = List(Date, Time, TxnCode, OfficeID, Country, Origin, Destination, RoundTrip, NbSegments, Seg1Departure, Seg1Arrival, Seg1Date, Seg1Carrier, Seg1BookingCode, Seg2Departure, Seg2Arrival, Seg2Date, Seg2Carrier, Seg2BookingCode, Seg3Departure, Seg3Arrival, Seg3Date, Seg3Carrier, Seg3BookingCode, Seg4Departure, Seg4Arrival, Seg4Date, Seg4Carrier, Seg4BookingCode, Seg5Departure, Seg5Arrival, Seg5Date, Seg5Carrier, Seg5BookingCode, Seg6Departure, Seg6Arrival, Seg6Date, Seg6Car...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "List(Date, Time, TxnCode, OfficeID, Country, Origin, Destination, RoundTrip, NbSegments, Seg1Departure, Seg1Arrival, Seg1Date, Seg1Carrier, Seg1BookingCode, Seg2Departure, Seg2Arrival, Seg2Date, Seg2Carrier, Seg2BookingCode, Seg3Departure, Seg3Arrival, Seg3Date, Seg3Carrier, Seg3BookingCode, Seg4Departure, Seg4Arrival, Seg4Date, Seg4Carrier, Seg4BookingCode, Seg5Departure, Seg5Arrival, Seg5Date, Seg5Carrier, Seg5BookingCode, Seg6Departure, Seg6Arrival, Seg6Date, Seg6Carrier, Seg6BookingCode, From, IsPublishedForNeg, IsFromInternet, IsFromVista, TerminalID, InternetOffice)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val searches = sc.textFile(searches_path)//Load RDD 1\n",
    "val bookings = sc.textFile(bookings_path)//Load RDD 2\n",
    "\n",
    "val headerColumns_searches = searches.first().split(\"\\\\^\").to[List]\n",
    "val head_searches = sc.textFile(searches_path).first()\n",
    "\n",
    "val headerColumns_bookings = bookings.first().split(\"\\\\^\").to[List]\n",
    "val head_bookings = sc.textFile(bookings_path).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "act_date           ^source^pos_ctry^pos_iata^pos_oid  ^rloc          ^cre_date           ^duration^distance^dep_port^dep_city^dep_ctry^arr_port^arr_city^arr_ctry^lst_port^lst_city^lst_ctry^brd_port^brd_city^brd_ctry^off_port^off_city^off_ctry^mkt_port^mkt_city^mkt_ctry^intl^route          ^carrier^bkg_class^cab_class^brd_time           ^off_time           ^pax^year^month^oid      "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_bookings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 4:=====================================================>   (14 + 1) / 15]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "searches_count = 20390199\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "20390199"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val searches_count = searches.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 6:=====================================================>   (16 + 1) / 17]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "bookings_count = 10000011\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "10000011"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val bookings_count = bookings.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Top 10 arrival airports in the world in 2013 (using the bookings file)\n",
    "Arrival airport is the column arr_port. It is the IATA code for the airport\n",
    "\n",
    "To get the total number of passengers for an airport, you can sum the column pax, grouping by arr_port. Note that there is negative pax. That corresponds to cancelations. So to get the total number of passengers that have actually booked, you should sum including the negatives (that will remove the canceled bookings).\n",
    "\n",
    "Print the top 10 arrival airports in the standard output, including the number of passengers.\n",
    "\n",
    "Bonus point: Get the name of the city or airport corresponding to that airport (programatically, we suggest to have a look at GeoBases in Github)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "parseLineBookings: (line: String)(String, String)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "/*Lets define a function to take each line and return what we want\n",
    " * In particular, we want the column 12 with arr_port and 34 with pax\n",
    " * VIP: to handle the wrong lines reading (ex. 14 fields) we need to cover that exception, \n",
    " * otherwise it would corrupt the RDD and give many headaches...\n",
    " */\n",
    "def parseLineBookings(line:String) = {\n",
    "    val fields   = line.split(\"\\\\^\")\n",
    "    //check if the line is complete..\n",
    "    if (fields.length>34){\n",
    "    val arr_port = fields(12).trim\n",
    "    val paxstring = fields(34).trim\n",
    "    if (arr_port==\"\" || paxstring==\"\") (\"KKK\",\"0\") else\n",
    "    (arr_port, paxstring)\n",
    "    }\n",
    "    else (\"KKK\",\"0\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Syntax Error.\n",
       "Message: \n",
       "StackTrace: "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//val pair_rdd = bookings.mapPartitionsWithIndex{(idx, iter) => if (idx == 0) iter.drop(1) else iter}.map(parseLineBookings).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pair_rdd = MapPartitionsRDD[9] at map at <console>:37\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[9] at map at <console>:37"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pair_rdd = bookings.map(parseLineBookings).persist(MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(arr_port,pax), (LHR,-1), (CLT,1), (CLT,1), (SVO,1), (SVO,1), (LGA,1), (LGA,1), (SIN,2), (SIN,2)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pair_rdd_2 = MapPartitionsRDD[12] at filter at <console>:39\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[12] at filter at <console>:39"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pair_rdd_2 = pair_rdd.filter(_._1 != \"arr_port\" ).persist(MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ShuffledRDD[11] at groupByKey at <console>:42"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_rdd_2.groupByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 8:=====================================================>   (16 + 1) / 17]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "kk = Array((KKK,0))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[(KKK,0)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val kk = pair_rdd.filter(_._1 != \"arr_port\" ).persist(MEMORY_AND_DISK).filter(_._1 == \"KKK\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pair_rdd_good = MapPartitionsRDD[17] at filter at <console>:39\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[17] at filter at <console>:39"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pair_rdd_good = pair_rdd.filter(_._1 != \"arr_port\" ).filter(_._1 != \"KKK\").persist(MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 9:=====================================================>   (16 + 1) / 17]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10000009"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_rdd_good.count()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "//CLEANUP\n",
    "val pair_rdd_3  = pair_rdd_2.filter(_._1 != \"\" )\n",
    "                 .filter(_._1.toUpperCase != \"NULL\")\n",
    "                 .filter(_._2.toUpperCase.trim != \"\" )\n",
    "                 .filter(_._2.toUpperCase.trim != \"NULL\")\n",
    "                 .persist(MEMORY_AND_DISK)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toInt: (s: String)Option[Int]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def toInt(s: String): Option[Int] = {\n",
    "  try {\n",
    "    Some(s.toInt)\n",
    "  } catch {\n",
    "    case e: Exception => None\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pair_bookings = MapPartitionsRDD[18] at map at <console>:43\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[18] at map at <console>:43"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Make sure wee have integers\n",
    "val pair_bookings = pair_rdd_good.map(x=> {\n",
    "      val tst = toInt(x._2)\n",
    "      tst  match {\n",
    "      case None => (x._1, 0)\n",
    "      case Some(y) => (x._1, y) }\n",
    "}).persist(MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(LHR,-1), (CLT,1), (CLT,1), (SVO,1), (SVO,1), (LGA,1), (LGA,1), (SIN,2), (SIN,2), (SIN,2)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_bookings.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "temp = ShuffledRDD[19] at reduceByKey at <console>:45\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ShuffledRDD[19] at reduceByKey at <console>:45"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val temp = pair_bookings.reduceByKey(_+_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 11:====================================================>   (16 + 1) / 17]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(IXS,390), (ELS,1370), (YQG,160), (SUV,220), (LFW,370), (DJE,1790), (PCL,210), (SBP,300), (DAL,1390), (TLC,430)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "top10 = Array((88809,LHR), (70930,MCO), (70530,LAX), (69630,LAS), (66270,JFK), (64490,CDG), (59460,BKK), (58150,MIA), (58000,SFO), (55590,DXB))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[(88809,LHR), (70930,MCO), (70530,LAX), (69630,LAS), (66270,JFK), (64490,CDG), (59460,BKK), (58150,MIA), (58000,SFO), (55590,DXB)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val top10 = temp.map(x => (x._2,x._1)).sortByKey(false).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(88809,LHR)\n",
      "(70930,MCO)\n",
      "(70530,LAX)\n",
      "(69630,LAS)\n",
      "(66270,JFK)\n",
      "(64490,CDG)\n",
      "(59460,BKK)\n",
      "(58150,MIA)\n",
      "(58000,SFO)\n",
      "(55590,DXB)\n"
     ]
    }
   ],
   "source": [
    "top10.foreach(e => println(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 9:=================>                                        (5 + 4) / 17]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.SparkException\n",
       "Message: Job aborted due to stage failure: Task 8 in stage 9.0 failed 1 times, most recent failure: Lost task 8.0 in stage 9.0 (TID 63, localhost, executor driver): java.lang.ArrayIndexOutOfBoundsException: 34\n",
       "\tat $line47.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.parseLineBookings(<console>:36)\n",
       "\tat $line60.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(<console>:37)\n",
       "\tat $line60.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(<console>:37)\n",
       "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n",
       "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)\n",
       "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:957)\n",
       "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)\n",
       "\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)\n",
       "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)\n",
       "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)\n",
       "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\n",
       "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)\n",
       "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:957)\n",
       "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)\n",
       "\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)\n",
       "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)\n",
       "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)\n",
       "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\n",
       "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
       "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
       "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
       "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)\n",
       "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:957)\n",
       "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)\n",
       "\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)\n",
       "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)\n",
       "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)\n",
       "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
       "\tat java.lang.Thread.run(Thread.java:748)\n",
       "\n",
       "Driver stacktrace:\n",
       "StackTrace: \tat parseLineBookings(<console>:36)\n",
       "\tat $anonfun$1.apply(<console>:37)\n",
       "\tat $anonfun$1.apply(<console>:37)\n",
       "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n",
       "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)\n",
       "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:957)\n",
       "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)\n",
       "\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)\n",
       "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)\n",
       "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)\n",
       "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\n",
       "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)\n",
       "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:957)\n",
       "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)\n",
       "\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)\n",
       "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)\n",
       "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)\n",
       "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\n",
       "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
       "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
       "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
       "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)\n",
       "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:957)\n",
       "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)\n",
       "\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)\n",
       "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)\n",
       "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)\n",
       "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
       "\tat java.lang.Thread.run(Thread.java:748)\n",
       "Driver stacktrace:\n",
       "  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n",
       "  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
       "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n",
       "  at scala.Option.foreach(Option.scala:257)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n",
       "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1958)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:935)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n",
       "  at org.apache.spark.rdd.RDD.collect(RDD.scala:934)\n",
       "  at org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:266)\n",
       "  at org.apache.spark.RangePartitioner.<init>(Partitioner.scala:128)\n",
       "  at org.apache.spark.rdd.OrderedRDDFunctions$$anonfun$sortByKey$1.apply(OrderedRDDFunctions.scala:62)\n",
       "  at org.apache.spark.rdd.OrderedRDDFunctions$$anonfun$sortByKey$1.apply(OrderedRDDFunctions.scala:61)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n",
       "  at org.apache.spark.rdd.OrderedRDDFunctions.sortByKey(OrderedRDDFunctions.scala:61)\n",
       "  ... 44 elided\n",
       "Caused by: java.lang.ArrayIndexOutOfBoundsException: 34\n",
       "  at parseLineBookings(<console>:36)\n",
       "  at $anonfun$1.apply(<console>:37)\n",
       "  at $anonfun$1.apply(<console>:37)\n",
       "  at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n",
       "  at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)\n",
       "  at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:957)\n",
       "  at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)\n",
       "  at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)\n",
       "  at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)\n",
       "  at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)\n",
       "  at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\n",
       "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\n",
       "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
       "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)\n",
       "  at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:957)\n",
       "  at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)\n",
       "  at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)\n",
       "  at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)\n",
       "  at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)\n",
       "  at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\n",
       "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\n",
       "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
       "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
       "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
       "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
       "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
       "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
       "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
       "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
       "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
       "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
       "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:336)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:334)\n",
       "  at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:957)\n",
       "  at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)\n",
       "  at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)\n",
       "  at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)\n",
       "  at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)\n",
       "  at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\n",
       "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\n",
       "  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n",
       "  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
       "  at org.apache.spark.scheduler.Task.run(Task.scala:99)\n",
       "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val arr_port_orderedByPax = mypair.sortByKey(false).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Unknown Error\n",
       "Message: lastException: Throwable = null\n",
       "<console>:29: error: not found: value arr_port_orderedByPax\n",
       "       arr_port_orderedByPax\n",
       "       ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr_port_orderedByPax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 2:=====================================================>   (16 + 1) / 17]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10000010"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_bookings.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "kk = MapPartitionsRDD[39] at map at <console>:39\n",
       "kk2 = MapPartitionsRDD[41] at filter at <console>:44\n",
       "kk3 = MapPartitionsRDD[42] at map at <console>:45\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[42] at map at <console>:45"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val kk = rdd_bookings.map(entry=>{\n",
    "    val arr = entry(12);\n",
    "    val pax = entry(34);\n",
    "    (arr,pax)\n",
    "}).persist(DISK_ONLY)\n",
    "val kk2 = kk.filter(_._1 != \" \").filter(_._2 != null) \n",
    "val kk3 = kk2.map{case (key,value) => (key.toString, value.toInt)}.persist(DISK_ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "kk3 = ShuffledRDD[43] at groupByKey at <console>:38\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ShuffledRDD[43] at groupByKey at <console>:38"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val kk3 = kk.groupByKey().persist(DISK_ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 19:==========================>                              (8 + 2) / 17]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.SparkException\n",
       "Message: Job aborted due to stage failure: Task 8 in stage 19.0 failed 1 times, most recent failure: Lost task 8.0 in stage 19.0 (TID 109, localhost, executor driver): java.lang.ArrayIndexOutOfBoundsException: 34\n",
       "\tat $line183.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(<console>:41)\n",
       "\tat $line183.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(<console>:39)\n",
       "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n",
       "\tat org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:141)\n",
       "\tat org.apache.spark.serializer.SerializerManager.dataSerializeStream(SerializerManager.scala:171)\n",
       "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1$$anonfun$apply$8.apply(BlockManager.scala:992)\n",
       "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1$$anonfun$apply$8.apply(BlockManager.scala:991)\n",
       "\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:57)\n",
       "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:991)\n",
       "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)\n",
       "\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)\n",
       "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)\n",
       "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)\n",
       "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
       "\tat java.lang.Thread.run(Thread.java:748)\n",
       "\n",
       "Driver stacktrace:\n",
       "StackTrace: \tat $anonfun$1.apply(<console>:41)\n",
       "\tat $anonfun$1.apply(<console>:39)\n",
       "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n",
       "\tat org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:141)\n",
       "\tat org.apache.spark.serializer.SerializerManager.dataSerializeStream(SerializerManager.scala:171)\n",
       "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1$$anonfun$apply$8.apply(BlockManager.scala:992)\n",
       "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1$$anonfun$apply$8.apply(BlockManager.scala:991)\n",
       "\tat org.apache.spark.storage.DiskStore.put(DiskStore.scala:57)\n",
       "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:991)\n",
       "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)\n",
       "\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)\n",
       "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)\n",
       "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)\n",
       "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
       "\tat java.lang.Thread.run(Thread.java:748)\n",
       "Driver stacktrace:\n",
       "  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n",
       "  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
       "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n",
       "  at scala.Option.foreach(Option.scala:257)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n",
       "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1353)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n",
       "  at org.apache.spark.rdd.RDD.take(RDD.scala:1326)\n",
       "  ... 44 elided\n",
       "Caused by: java.lang.ArrayIndexOutOfBoundsException: 34\n",
       "  at $anonfun$1.apply(<console>:41)\n",
       "  at $anonfun$1.apply(<console>:39)\n",
       "  at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n",
       "  at org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:141)\n",
       "  at org.apache.spark.serializer.SerializerManager.dataSerializeStream(SerializerManager.scala:171)\n",
       "  at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1$$anonfun$apply$8.apply(BlockManager.scala:992)\n",
       "  at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1$$anonfun$apply$8.apply(BlockManager.scala:991)\n",
       "  at org.apache.spark.storage.DiskStore.put(DiskStore.scala:57)\n",
       "  at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:991)\n",
       "  at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)\n",
       "  at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)\n",
       "  at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)\n",
       "  at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)\n",
       "  at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\n",
       "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\n",
       "  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n",
       "  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
       "  at org.apache.spark.scheduler.Task.run(Task.scala:99)\n",
       "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kk3.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ind_bookings: (field: String)Int\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "34\n"
     ]
    }
   ],
   "source": [
    "def ind_bookings(field: String): Int = headerColumns_bookings.indexOf(field) \n",
    "println(ind_bookings(\"arr_port\"))\n",
    "println(ind_bookings(\"pax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_pair_ex1 = MapPartitionsRDD[8] at map at <console>:39\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[8] at map at <console>:39"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df_pair_ex1 = rdd_bookings.map(l => (l(12),l(34)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ex1 = ShuffledRDD[11] at groupByKey at <console>:41\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ShuffledRDD[11] at groupByKey at <console>:41"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ex1 = df_pair_ex1.filter((_._2.toInt < 0) ).filter(_._2.toInt >= 0).groupByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ex11 = MapPartitionsRDD[12] at mapValues at <console>:43\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[12] at mapValues at <console>:43"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ex11 = ex1.mapValues(v=>(v.map(x => x.toInt)).sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 4:===========================>                              (8 + 1) / 17]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.SparkException\n",
       "Message: Job aborted due to stage failure: Task 8 in stage 4.0 failed 1 times, most recent failure: Lost task 8.0 in stage 4.0 (TID 42, localhost, executor driver): java.lang.IndexOutOfBoundsException: 34\n",
       "\tat scala.collection.LinearSeqOptimized$class.apply(LinearSeqOptimized.scala:65)\n",
       "\tat scala.collection.immutable.List.apply(List.scala:84)\n",
       "\tat $line44.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(<console>:39)\n",
       "\tat $line44.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(<console>:39)\n",
       "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n",
       "\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462)\n",
       "\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461)\n",
       "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
       "\tat java.lang.Thread.run(Thread.java:748)\n",
       "\n",
       "Driver stacktrace:\n",
       "StackTrace: \tat scala.collection.LinearSeqOptimized$class.apply(LinearSeqOptimized.scala:65)\n",
       "\tat scala.collection.immutable.List.apply(List.scala:84)\n",
       "\tat $anonfun$1.apply(<console>:39)\n",
       "\tat $anonfun$1.apply(<console>:39)\n",
       "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n",
       "\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462)\n",
       "\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461)\n",
       "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
       "\tat java.lang.Thread.run(Thread.java:748)\n",
       "Driver stacktrace:\n",
       "  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n",
       "  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
       "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n",
       "  at scala.Option.foreach(Option.scala:257)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n",
       "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1353)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n",
       "  at org.apache.spark.rdd.RDD.take(RDD.scala:1326)\n",
       "  ... 44 elided\n",
       "Caused by: java.lang.IndexOutOfBoundsException: 34\n",
       "  at scala.collection.LinearSeqOptimized$class.apply(LinearSeqOptimized.scala:65)\n",
       "  at scala.collection.immutable.List.apply(List.scala:84)\n",
       "  at $anonfun$1.apply(<console>:39)\n",
       "  at $anonfun$1.apply(<console>:39)\n",
       "  at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n",
       "  at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462)\n",
       "  at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461)\n",
       "  at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)\n",
       "  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n",
       "  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
       "  at org.apache.spark.scheduler.Task.run(Task.scala:99)\n",
       "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ex12 = ex11.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check reading was correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n",
      "List(date, time, txncode, officeid, country, origin, destination, roundtrip, nbsegments, seg1departure, seg1arrival, seg1date, seg1carrier, seg1bookingcode, seg2departure, seg2arrival, seg2date, seg2carrier, seg2bookingcode, seg3departure, seg3arrival, seg3date, seg3carrier, seg3bookingcode, seg4departure, seg4arrival, seg4date, seg4carrier, seg4bookingcode, seg5departure, seg5arrival, seg5date, seg5carrier, seg5bookingcode, seg6departure, seg6arrival, seg6date, seg6carrier, seg6bookingcode, from, ispublishedforneg, isfrominternet, isfromvista, terminalid, internetoffice)\n",
      "List(2013-01-01, 20:25:57, MPT, 624d8c3ac0b3a7ca03e3c167e0f48327, DE, TXL, AUH, 1, 2, TXL, AUH, 2013-01-26, D2, , AUH, TXL, 2013-02-02, D2, , , , , , , , , , , , , , , , , , , , , , 1ASIWS, 0, 0, 0, d41d8cd98f00b204e9800998ecf8427e, FRA)\n"
     ]
    }
   ],
   "source": [
    "println(rdd_searches.first().length==head_searches.length)\n",
    "println(head_searches)\n",
    "println(rdd_searches.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n",
      "List(act_date, source, pos_ctry, pos_iata, pos_oid, rloc, cre_date, duration, distance, dep_port, dep_city, dep_ctry, arr_port, arr_city, arr_ctry, lst_port, lst_city, lst_ctry, brd_port, brd_city, brd_ctry, off_port, off_city, off_ctry, mkt_port, mkt_city, mkt_ctry, intl, route, carrier, bkg_class, cab_class, brd_time, off_time, pax, year, month, oid)\n",
      "List(2013-03-05 00:00:00, 1A, DE, a68dd7ae953c8acfb187a1af2dcbe123, 1a11ae49fcbf545fd2afc1a24d88d2b7, ea65900e72d71f4626378e2ebd298267, 2013-02-22 00:00:00, 1708, 0, ZRH, ZRH, CH, LHR, LON, GB, ZRH, ZRH, CH, LHR, LON, GB, ZRH, ZRH, CH, LHRZRH, LONZRH, CHGB, 1, LHRZRH, VI, T, Y, 2013-03-07 08:50:00, 2013-03-07 11:33:37, -1, 2013, 3, NULL)\n"
     ]
    }
   ],
   "source": [
    "println(rdd_bookings.first().length==head_bookings.length)\n",
    "println(head_bookings)\n",
    "println(rdd_bookings.first())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Count number of lines on files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20390199\n",
      "10000011\n"
     ]
    }
   ],
   "source": [
    "//Print number of lines\n",
    "println(searches_count)\n",
    "println(bookings_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This takes a long time to run (about 4 minutes each counting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I have some problem with the code above and It is a bit inefficient to work with such a big RDD, instead we would read directly the columns we need for each task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Read first rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "first_searches = Date^Time^TxnCode^OfficeID^Country^Origin^Destination^RoundTrip^NbSegments^Seg1Departure^Seg1Arrival^Seg1Date^Seg1Carrier^Seg1BookingCode^Seg2Departure^Seg2Arrival^Seg2Date^Seg2Carrier^Seg2BookingCode^Seg3Departure^Seg3Arrival^Seg3Date^Seg3Carrier^Seg3BookingCode^Seg4Departure^Seg4Arrival^Seg4Date^Seg4Carrier^Seg4BookingCode^Seg5Departure^Seg5Arrival^Seg5Date^Seg5Carrier^Seg5BookingCode^Seg6Departure^Seg6Arrival^Seg6Date^Seg6Carrier^Seg6BookingCode^From^IsPublishedForNeg^IsFromInternet^IsFromVista^TerminalID^InternetOffice\n",
       "first_bookings = \"act_date           ^source^pos_ctry^pos_iata^pos_oid  ^rloc          ^cre_date           ^duration^distance^dep_port^dep_city^dep_ctry^arr_port^arr_city^arr_ctry^lst_port^lst_city^lst_ctry^brd_port^brd_city^brd_ctry^o...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "act_date           ^source^pos_ctry^pos_iata^pos_oid  ^rloc          ^cre_date           ^duration^distance^dep_port^dep_city^dep_ctry^arr_port^arr_city^arr_ctry^lst_port^lst_city^lst_ctry^brd_port^brd_city^brd_ctry^off_port^off_city^off_ctry^mkt_port^mkt_city^mkt_ctry^intl^route          ^carrier^bkg_class^cab_class^brd_time           ^off_time           ^pax^year^month^oid      "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val first_searches = sc.textFile(searches_path).first//Take only first row and c\n",
    "val first_bookings = sc.textFile(bookings_path).first//Take only first row and c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the separator is the character \"\\\\^\". So now we will create a list with the trimmed field names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lf_searches = List(Date, Time, TxnCode, OfficeID, Country, Origin, Destination, RoundTrip, NbSegments, Seg1Departure, Seg1Arrival, Seg1Date, Seg1Carrier, Seg1BookingCode, Seg2Departure, Seg2Arrival, Seg2Date, Seg2Carrier, Seg2BookingCode, Seg3Departure, Seg3Arrival, Seg3Date, Seg3Carrier, Seg3BookingCode, Seg4Departure, Seg4Arrival, Seg4Date, Seg4Carrier, Seg4BookingCode, Seg5Departure, Seg5Arrival, Seg5Date, Seg5Carrier, Seg5BookingCode, Seg6Departure, Seg6Arrival, Seg6Date, Seg6Carrier, Seg6BookingCode, From, IsPublishedForNeg, IsFromInternet, IsFromVista, TerminalID, InternetOffice)\n",
       "lf_bookings = List(act_date, source, pos_ctry, pos_iata, pos_oid, rloc, cre_date, duration, distance, dep_port, dep_city, dep_ctry, arr_port, arr_city, arr_ctry, lst_port, lst_...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "List(act_date, source, pos_ctry, pos_iata, pos_oid, rloc, cre_date, duration, distance, dep_port, dep_city, dep_ctry, arr_port, arr_city, arr_ctry, lst_port, lst_city, lst_ctry, brd_port, brd_city, brd_ctry, off_port, off_city, off_ctry, mkt_port, mkt_city, mkt_ctry, intl, route, carrier, bkg_class, cab_class, brd_time, off_time, pax, year, month, oid)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lf_searches = first_searches.split(\"\\\\^\").map(field=>field.trim).to[List]\n",
    "val lf_bookings = first_bookings.split(\"\\\\^\").map(field=>field.trim).to[List]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will create a dictionary to be able to refer to the column just by using the field name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dsearch = Map(TxnCode -> 2, Seg6Departure -> 34, TerminalID -> 43, Country -> 4, Seg1Carrier -> 12, Seg3Departure -> 19, Seg5Departure -> 29, Origin -> 5, RoundTrip -> 7, OfficeID -> 3, Seg2Arrival -> 15, InternetOffice -> 44, Seg5Carrier -> 32, Seg3Date -> 21, Seg5BookingCode -> 33, Seg2Departure -> 14, Seg1BookingCode -> 13, Seg4Departure -> 24, Seg6Carrier -> 37, Seg6Arrival -> 35, IsFromVista -> 42, Seg4Date -> 26, IsPublishedForNeg -> 40, Seg4Arrival -> 25, IsFromInternet -> 41, Seg1Arrival -> 10, Date -> 0, Seg2Carrier -> 17, Seg4Carrier -> 27, Seg6BookingCode -> 38, Seg3Arrival -> 20, NbSegments -> 8, Destination -> 6, Seg1Departure -> 9, Seg1Date -> 11, Seg3BookingCode -> 23, Seg3Carrier -> 22, Seg5Date -> 31, Seg2BookingCode -> 18, Seg5Arrival -> 30, Seg6Date -...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map(TxnCode -> 2, Seg6Departure -> 34, TerminalID -> 43, Country -> 4, Seg1Carrier -> 12, Seg3Departure -> 19, Seg5Departure -> 29, Origin -> 5, RoundTrip -> 7, OfficeID -> 3, Seg2Arrival -> 15, InternetOffice -> 44, Seg5Carrier -> 32, Seg3Date -> 21, Seg5BookingCode -> 33, Seg2Departure -> 14, Seg1BookingCode -> 13, Seg4Departure -> 24, Seg6Carrier -> 37, Seg6Arrival -> 35, IsFromVista -> 42, Seg4Date -> 26, IsPublishedForNeg -> 40, Seg4Arrival -> 25, IsFromInternet -> 41, Seg1Arrival -> 10, Date -> 0, Seg2Carrier -> 17, Seg4Carrier -> 27, Seg6BookingCode -> 38, Seg3Arrival -> 20, NbSegments -> 8, Destination -> 6, Seg1Departure -> 9, Seg1Date -> 11, Seg3BookingCode -> 23, Seg3Carrier -> 22, Seg5Date -> 31, Seg2BookingCode -> 18, Seg5Arrival -> 30, Seg6Date -> 36, From -> 39, Time -> 1, Seg4BookingCode -> 28, Seg2Date -> 16)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dsearch: Map[String, Int] = Map(lf_searches map {s => (s, lf_searches.indexOf(s))} : _*);//searchs dictionary\n",
    "val dbook: Map[String, Int] = Map(lf_bookings map {s => (s, lf_bookings.indexOf(s))} : _*);//bookings dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbook(\"pax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbook(\"arr_port\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can read bookings file selecting for instance the columns of arr_port and pax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.{RangePartitioner,HashPartitioner, SparkContext}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! now we can use the transformations and methods for Pair RDDs!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "head1 = \"act_date           ^source^pos_ctry^pos_iata^pos_oid  ^rloc          ^cre_date           ^duration^distance^dep_port^dep_city^dep_ctry^arr_port^arr_city^arr_ctry^lst_port^lst_city^lst_ctry^brd_port^brd_city^brd_ctry^off_port^off_city^off_ctry^mkt_port^mkt_city^mkt_ctry^intl^route          ^carrier^bkg_class^cab_class^brd_time           ^off_time           ^pax^year^month^oid      \"\n",
       "rdd1 = MapPartitionsRDD[15] at map at <console>:40\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[15] at map at <console>:40"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val root_path: String = \"/media/sf_VirtualFolder/amadeus/\";//where to find files\n",
    "val searches_path: String = root_path + \"searches.csv.bz2\";\n",
    "val bookings_path: String = root_path + \"bookings.csv.bz2\";\n",
    "//Now drop first line, split lines with ^ separator and trim words\n",
    "val head1 = sc.textFile(bookings_path).first\n",
    "val rdd1 = sc.textFile(bookings_path).filter(_!=head1).map(line => (line.split(\"\\\\^\")(12).trim, line.split(\"\\\\^\")(34).toInt)).persist()\n",
    "val rdd2 = rdd1.filter(_._2!=\"\").filter(!_._1.isEmpty).filter(_._2!=null).filter(_._1!=null)\n",
    "//rdd1.count()\n",
    "rdd2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd2 = MapPartitionsRDD[19] at filter at <console>:39\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[19] at filter at <console>:39"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd2 = rdd1.filter(_._2!=\"\").filter(!_._1.isEmpty).filter(_._2!=null).filter(_._1!=null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(LHR,-1), (CLT,1), (CLT,1), (SVO,1), (SVO,1), (LGA,1), (LGA,1), (SIN,2), (SIN,2), (SIN,2)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 6:=================>                                        (5 + 4) / 17]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lastException = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.SparkException\n",
       "Message: Job aborted due to stage failure: Task 8 in stage 6.0 failed 1 times, most recent failure: Lost task 8.0 in stage 6.0 (TID 35, localhost, executor driver): java.lang.ArrayIndexOutOfBoundsException: 34\n",
       "\tat $line46.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$2.apply(<console>:40)\n",
       "\tat $line46.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$2.apply(<console>:40)\n",
       "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n",
       "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)\n",
       "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:957)\n",
       "\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)\n",
       "\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)\n",
       "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)\n",
       "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)\n",
       "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:748)\n",
       "\n",
       "Driver stacktrace:\n",
       "StackTrace: \tat $anonfun$2.apply(<console>:40)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n",
       "  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
       "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n",
       "  at scala.Option.foreach(Option.scala:257)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n",
       "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1958)\n",
       "  at org.apache.spark.rdd.RDD.count(RDD.scala:1157)\n",
       "  ... 44 elided\n",
       "Caused by: java.lang.ArrayIndexOutOfBoundsException: 34\n",
       "  at $anonfun$2.apply(<console>:40)\n",
       "  at $anonfun$2.apply(<console>:40)\n",
       "  at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n",
       "  at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)\n",
       "  at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:957)\n",
       "  at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:948)\n",
       "  at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:888)\n",
       "  at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:948)\n",
       "  at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:694)\n",
       "  at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\n",
       "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\n",
       "  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
       "  at org.apache.spark.scheduler.Task.run(Task.scala:99)\n",
       "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 22:======>                                                  (2 + 8) / 17]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lastException = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.SparkException\n",
       "Message: Job aborted due to stage failure: Task 8 in stage 22.0 failed 1 times, most recent failure: Lost task 8.0 in stage 22.0 (TID 82, localhost, executor driver): java.lang.ArrayIndexOutOfBoundsException\n",
       "\n",
       "Driver stacktrace:\n",
       "StackTrace:   at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n",
       "  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
       "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n",
       "  at scala.Option.foreach(Option.scala:257)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n",
       "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1958)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$foreach$1.apply(RDD.scala:917)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$foreach$1.apply(RDD.scala:915)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n",
       "  at org.apache.spark.rdd.RDD.foreach(RDD.scala:915)\n",
       "  ... 44 elided"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//val rdd3 = rdd1.filter(_._1!=null).filter(_._2!= null).map(x=>(x._1.trim().toUpperCase(), x._2.toInt)).reduceByKey(_+_)\n",
    "val rdd3 = rdd1.filter(_._1.length>0).filter(_._2!=null)\n",
    "//rdd1.filter(_._1 !=null).foreach(println)\n",
    "//val kk = rdd3.filter(_._2>10).take(10)\n",
    "\n",
    "//val rdd4 = rdd2.reduceByKey((x,y) => x + y )\n",
    "//.mapByValue(_+_)\n",
    "//.groupByKey\n",
    "//\n",
    "//rdd4.take(10)\n",
    "//val top10 = rdd3.sortByKey(false).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top10.forEach(println(_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 17:===>                                                     (1 + 8) / 17]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.SparkException\n",
       "Message: Job aborted due to stage failure: Task 8 in stage 17.0 failed 1 times, most recent failure: Lost task 8.0 in stage 17.0 (TID 57, localhost, executor driver): java.lang.ArrayIndexOutOfBoundsException\n",
       "\n",
       "Driver stacktrace:\n",
       "StackTrace: Driver stacktrace:\n",
       "  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n",
       "  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
       "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n",
       "  at scala.Option.foreach(Option.scala:257)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n",
       "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1981)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1025)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n",
       "  at org.apache.spark.rdd.RDD.reduce(RDD.scala:1007)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$max$1.apply(RDD.scala:1441)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n",
       "  at org.apache.spark.rdd.RDD.max(RDD.scala:1440)\n",
       "  ... 44 elided"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Top 10 arrival airports in the world in 2013 (using the bookings file)\n",
    "Arrival airport is the column arr_port. It is the IATA code for the airport\n",
    "\n",
    "To get the total number of passengers for an airport, you can sum the column pax, grouping by arr_port. Note that there is negative pax. That corresponds to cancelations. So to get the total number of passengers that have actually booked, you should sum including the negatives (that will remove the canceled bookings).\n",
    "\n",
    "Print the top 10 arrival airports in the standard output, including the number of passengers.\n",
    "\n",
    "Bonus point: Get the name of the city or airport corresponding to that airport (programatically, we suggest to have a look at GeoBases in Github)\n",
    "\n",
    "Bonus point: Solve this problem using pandas (instead of any other approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
